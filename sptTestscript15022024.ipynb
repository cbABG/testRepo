{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chandan.bhardwaj\\AppData\\Local\\Temp\\ipykernel_1704\\440697630.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load configuration from YAML file\n",
    "with open('configTesting15022024.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "proxies = {'http': config['proxies']['http']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_year(date_str):\n",
    "    \"\"\"Parse date string and adjust the year.\"\"\"\n",
    "    today = datetime.today()\n",
    "    month, day = map(int, date_str.split('/'))\n",
    "    year = today.year if month <= today.month else today.year - 1\n",
    "    return datetime(year=year, month=month, day=day).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved to combined_commodities_data.csv.\n"
     ]
    }
   ],
   "source": [
    "def fetch_commodity_data(website_config):\n",
    "    \"\"\"Fetch and process commodity data from a given website configuration.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(website_config['url'], proxies=proxies)\n",
    "        response.raise_for_status()  # This will raise an exception for HTTP error codes\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        data_rows = []\n",
    "        parent = soup.find(\"ul\", class_='zwd_table no_select').findAll(\"li\")\n",
    "        for ele in parent:\n",
    "            row_data = [p.get_text() for p in ele.findAll(\"p\")]\n",
    "            if row_data:  # Ensure row_data is not empty\n",
    "                data_rows.append(row_data)\n",
    "        \n",
    "        if data_rows:\n",
    "            df = pd.DataFrame(data_rows[1:], columns=data_rows[0])\n",
    "            df['Date'] = df['Date'].apply(find_year)\n",
    "            df['Price'] = df['Price'].astype(float)\n",
    "            df['Product Name'] = website_config['product_name']\n",
    "            df['Type'] = website_config['type']\n",
    "            df['Currency'] = website_config['currency']\n",
    "            df['WebsiteName'] = website_config['WebsiteName']\n",
    "            df['QueryTimestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            df['TableFetchTimestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch and process data for {website_config['product_name']}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of failure\n",
    "\n",
    "def run():\n",
    "    all_data_frames = []  # List to store all DataFrames\n",
    "    for website in config['websites']:\n",
    "        df = fetch_commodity_data(website)\n",
    "        if not df.empty:\n",
    "            all_data_frames.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    if all_data_frames:\n",
    "        combined_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        # Save the combined DataFrame to a single CSV file\n",
    "        combined_csv_filename = \"combined_commodities_data.csv\"\n",
    "        combined_df.to_csv(combined_csv_filename, index=False)\n",
    "        print(f\"All data saved to {combined_csv_filename}.\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is still work in progress!\n",
    "#def fetch_commodity_data(website_config):\n",
    "    \"\"\"Fetches and processes commodity data based on website configuration.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(website_config['url'], proxies=proxies)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        data_rows = []\n",
    "        # Placeholder for actual data extraction logic\n",
    "        parent = soup.find(\"ul\", class_='zwd_table no_select').findAll(\"li\")\n",
    "        for ele in parent:\n",
    "            row_data = [p.get_text() for p in ele.findAll(\"p\")]\n",
    "            if row_data:\n",
    "                data_rows.append(row_data)\n",
    "        \n",
    "        if data_rows:\n",
    "            df = pd.DataFrame(data_rows[1:], columns=['Date', 'Price', 'Open', 'High', 'Low', 'Volume', 'ChgPerc'])\n",
    "            df['Date'] = df['Date'].apply(lambda x: find_year(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            df['ProductName'] = website_config['product_name']\n",
    "            df['Type'] = website_config['type']\n",
    "            df['Currency'] = website_config['currency']\n",
    "            df['WebsiteName'] = website_config['WebsiteName']\n",
    "            df['QueryTimestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            df['TableFetchTimestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Placeholder for USD_INR and RMB_INR conversion rates, adjust as necessary\n",
    "            df['USD_INR'] = 0  # Placeholder value\n",
    "            df['RMB_INR'] = 0  # Placeholder value\n",
    "            # Generating IdentifierKey\n",
    "            df['IdentifierKey'] = df.apply(lambda row: f\"{row['TableFetchTimestamp']}_{row['ProductName']}_{row['WebsiteName']}\", axis=1)\n",
    "            \n",
    "            return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch and process data for {website_config['product_name']}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    combined_data = []\n",
    "    for website in config['websites']:\n",
    "        df = fetch_commodity_data(website)\n",
    "        if not df.empty:\n",
    "            combined_data.append(df)\n",
    "\n",
    "    if combined_data:\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_csv_filename = \"combined_commodities_data.csv\"\n",
    "        combined_df.to_csv(combined_csv_filename, index=False)\n",
    "        print(f\"Combined data saved to {combined_csv_filename}.\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to combined_commodities_data.csv.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_usd_inr_conversion_rate():\n",
    "    url = \"https://in.investing.com/currencies/usd-inr-historical-data\"\n",
    "    response = requests.get(url, proxies=proxies)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # You need to replace 'your-row-selector' with the actual selector that targets the rate\n",
    "    rows = soup.select('table.common-table.medium.js-table tr')\n",
    "    if rows:\n",
    "        latest_row = rows[1]  # Assuming the first row after the header has the latest rates\n",
    "        rate = latest_row.select_one('td:nth-child(2)').text  # Assuming the second column has the rate\n",
    "        return float(rate.replace(',', ''))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_rmb_inr_conversion_rate():\n",
    "    url = \"https://in.investing.com/currencies/cny-inr-historical-data\"\n",
    "    response = requests.get(url, proxies=proxies)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    rows = soup.select('table.common-table.medium.js-table tr')\n",
    "    if rows:\n",
    "        latest_row = rows[1]\n",
    "        rate = latest_row.select_one('td:nth-child(2)').text\n",
    "        return float(rate.replace(',', ''))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_commodity_data(website_config,usd_inr_rate, rmb_inr_rate):\n",
    "    \"\"\"Fetch and process commodity data based on website configuration.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(website_config['url'], proxies=proxies)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the data table, assuming 'li' elements under 'ul.zwd_table.no_select' contain the data rows\n",
    "        table_rows = soup.select('ul.zwd_table.no_select li')[1:]  # Skip the first row if it contains headers\n",
    "        \n",
    "        data_rows = []\n",
    "        for row in table_rows:\n",
    "            # Extract the text from each 'p' tag and strip it\n",
    "            columns = [col.get_text(strip=True) for col in row.find_all('p')]\n",
    "            # Now columns[0] should correspond to the 'Date' and columns[1] to the 'Price'\n",
    "            if len(columns) >= 2:  # Ensure there's at least Date and Price\n",
    "                date, price = columns[0], columns[1]\n",
    "                # Here you might need to convert date and price to the appropriate format\n",
    "                data_rows.append([\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # QueryTimestamp\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # TableFetchTimestamp\n",
    "                    price,  # Price from the page\n",
    "                    website_config['currency'],  # Currency from config\n",
    "                    'N/A', 'N/A', 'N/A', 'N/A', 'N/A',  # Other values as N/A or placeholders\n",
    "                    website_config['product_name'],  # ProductName from config\n",
    "                    website_config['type'],  # Type from config\n",
    "                    website_config['WebsiteName'],  # WebsiteName from config\n",
    "                    'N/A', 'N/A'  # USD_INR and RMB_INR as placeholders\n",
    "                ])\n",
    "        \n",
    "        # Construct DataFrame\n",
    "        df = pd.DataFrame(data_rows, columns=[\n",
    "            'QueryTimestamp', 'TableFetchTimestamp', 'Price', 'Currency', 'Open_', 'High', 'Low', \n",
    "            'Volume', 'ChgPerc', 'ProductName', 'Type', 'WebsiteName', 'USD_INR', 'RMB_INR'\n",
    "        ])\n",
    "        \n",
    "        # Assume that IdentifierKey is a combination of Timestamp, ProductName, and WebsiteName\n",
    "        df['IdentifierKey'] = df.apply(\n",
    "            lambda x: f\"{x['TableFetchTimestamp']}_{x['ProductName']}_{x['WebsiteName']}\", axis=1\n",
    "        )\n",
    "        df['USD_INR'] = usd_inr_rate\n",
    "        df['RMB_INR'] = rmb_inr_rate\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data for {website_config['product_name']}: {e}\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            'QueryTimestamp', 'TableFetchTimestamp', 'Price', 'Currency', 'Open_', 'High', 'Low', \n",
    "            'Volume', 'ChgPerc', 'ProductName', 'Type', 'WebsiteName', 'USD_INR', 'RMB_INR', 'IdentifierKey'\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    usd_inr_rate = fetch_usd_inr_conversion_rate()\n",
    "    rmb_inr_rate = fetch_rmb_inr_conversion_rate()\n",
    "    combined_data = []\n",
    "    for website in config['websites']:\n",
    "        df = fetch_commodity_data(website,usd_inr_rate, rmb_inr_rate)\n",
    "        if not df.empty:\n",
    "            combined_data.append(df)\n",
    "\n",
    "    if combined_data:\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_csv_filename = \"combined_commodities_data.csv\"\n",
    "        combined_df.to_csv(combined_csv_filename, index=False)\n",
    "        print(f\"Combined data saved to {combined_csv_filename}.\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to combined_commodities_data.csv.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_commodity_data(website_config, usd_inr_rate, rmb_inr_rate):\n",
    "    \"\"\"Fetch and process commodity data based on website configuration.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(website_config['url'], headers={'User-Agent': 'Mozilla/5.0'}, proxies=proxies)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        data_rows = []\n",
    "        table_rows = soup.select('ul.zwd_table.no_select li')[1:]  # Skip the first row if it contains headers\n",
    "        for row in table_rows:\n",
    "            columns = [col.get_text(strip=True) for col in row.find_all('p')]\n",
    "            if len(columns) >= 2:\n",
    "                date, price = columns[0], columns[1]\n",
    "                data_rows.append([\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # QueryTimestamp\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # TableFetchTimestamp\n",
    "                    price,  # Price from the page\n",
    "                    website_config['currency'],  # Currency from config\n",
    "                    'N/A', 'N/A', 'N/A', 'N/A', 'N/A',  # Other values as N/A or placeholders\n",
    "                    website_config['product_name'],  # ProductName from config\n",
    "                    website_config['type'],  # Type from config\n",
    "                    website_config['WebsiteName'],  # WebsiteName from config\n",
    "                    usd_inr_rate,  # USD_INR rate from fetched data\n",
    "                    rmb_inr_rate,  # RMB_INR rate from fetched data\n",
    "                ])\n",
    "\n",
    "        # Construct DataFrame\n",
    "        df = pd.DataFrame(data_rows, columns=[\n",
    "            'QueryTimestamp', 'TableFetchTimestamp', 'Price', 'Currency', 'Open_', 'High', 'Low',\n",
    "            'Volume', 'ChgPerc', 'ProductName', 'Type', 'WebsiteName', 'USD_INR', 'RMB_INR'\n",
    "        ])\n",
    "\n",
    "        # Generate IdentifierKey\n",
    "        df['IdentifierKey'] = df.apply(\n",
    "            lambda x: f\"{x['TableFetchTimestamp']}_{x['ProductName']}_{x['WebsiteName']}\", axis=1\n",
    "        )\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data for {website_config['product_name']}: {e}\")\n",
    "        return pd.DataFrame(columns=[\n",
    "            'QueryTimestamp', 'TableFetchTimestamp', 'Price', 'Currency', 'Open_', 'High', 'Low',\n",
    "            'Volume', 'ChgPerc', 'ProductName', 'Type', 'WebsiteName', 'USD_INR', 'RMB_INR', 'IdentifierKey'\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No element found for the selector table.common-table.medium.js-table tr:nth-of-type(2) td:nth-of-type(2) at URL https://in.investing.com/currencies/usd-inr-historical-data\n",
      "No element found for the selector table.common-table.medium.js-table tr:nth-of-type(2) td:nth-of-type(2) at URL https://in.investing.com/currencies/cny-inr-historical-data\n",
      "Failed to fetch conversion rates. Exiting.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_conversion_rate(url, selector):\n",
    "    \"\"\"Fetch the latest conversion rate from a given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, proxies=proxies)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rate_cell = soup.select_one(selector)\n",
    "        if rate_cell:\n",
    "            rate_text = rate_cell.get_text(strip=True).replace(',', '')\n",
    "            return float(rate_text)\n",
    "        else:\n",
    "            print(f\"No element found for the selector {selector} at URL {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching the conversion rate from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def run():\n",
    "    # Define the selectors for the conversion rates on the Investing.com website\n",
    "    usd_inr_selector = 'table.common-table.medium.js-table tr:nth-of-type(2) td:nth-of-type(2)'\n",
    "    rmb_inr_selector = 'table.common-table.medium.js-table tr:nth-of-type(2) td:nth-of-type(2)'\n",
    "    \n",
    "    # Fetch the conversion rates using the defined selectors\n",
    "    usd_inr_rate = fetch_conversion_rate(\"https://in.investing.com/currencies/usd-inr-historical-data\", usd_inr_selector)\n",
    "    rmb_inr_rate = fetch_conversion_rate(\"https://in.investing.com/currencies/cny-inr-historical-data\", rmb_inr_selector)\n",
    "\n",
    "    # Make sure the rates were fetched successfully, otherwise set them to None or handle as needed\n",
    "    if usd_inr_rate is None or rmb_inr_rate is None:\n",
    "        print(\"Failed to fetch conversion rates. Exiting.\")\n",
    "        return  # or you can set a default value or handle this case as needed\n",
    "\n",
    "    combined_data = []\n",
    "    for website in config['websites']:\n",
    "        df = fetch_commodity_data(website, usd_inr_rate, rmb_inr_rate)\n",
    "        if not df.empty:\n",
    "            combined_data.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one if any data was fetched\n",
    "    if combined_data:\n",
    "        combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "        combined_csv_filename = \"combined_commodities_data.csv\"\n",
    "        combined_df.to_csv(combined_csv_filename, index=False)\n",
    "        print(f\"Combined data saved to {combined_csv_filename}.\")\n",
    "    else:\n",
    "        print(\"No data fetched.\")\n",
    "\n",
    "# The entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_conversion_rate(url, selector):\n",
    "    \"\"\"Fetch the latest conversion rate from a given URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, proxies=proxies)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rate_cell = soup.select_one(selector)\n",
    "        if rate_cell:\n",
    "            rate_text = rate_cell.get_text(strip=True).replace(',', '')\n",
    "            return float(rate_text)\n",
    "        else:\n",
    "            print(f\"No element found for the selector {selector} at URL {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching the conversion rate from {url}: {e}\")\n",
    "        return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
